{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BANTS theory documentation\n",
    "\n",
    "In these notes, I'll introduce the basic concepts behind dynamic Bayesian networks for time series prediction and use them to demonstrate how the `bants` class is structured. Each proceeding section of these notes will cover a different network structure that can be called when initialising the `bants` class like so: `bts = bants('AR-GP')`. So far the only mode is `'AR-GP'`, which is described below.\n",
    "\n",
    "### Dynamic Bayesian network structure\n",
    "\n",
    "A dynamic Bayesian network is denoted ${\\rm DBN}(G,p)$ where $G$ denotes the graph structure and $p$ denotes the joint probability distribution over vertices. The latter, for the observation vector of an $N$-dimensional time series process $x_{t}$ (with $i$ indexing an element of the vector $x$ and $t$ denoting a discrete observation timepoint index) is factorisable with respect to the graph structure in the following manner \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(x_{t},x_{t-1},\\dots ,x_{1}) = \\prod_{n=0}^{t-1}p[x_{t-n}\\vert {\\rm pa}(x_{t-n})]\\,,\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where ${\\rm pa}(x_{t})$ denotes the parent nodes of $x_{t}$ which are defined by $G$. Choosing a set of parameters $\\Theta$ (defined within their prior domain $\\Omega_\\Theta$), graph $G$, and an observation of $x_t$, Bayes' rule yields the following for the posterior distribution over $\\Theta$ at timepoint $t$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\cal P}(\\Theta \\vert G,x_t) = \\frac{P(x_t,\\Theta \\vert G )}{\\int_{\\Theta \\in \\Omega_\\Theta} {\\rm d}\\Theta P(x_t,\\Theta \\vert G )} = \\frac{{\\cal L}(x_t\\vert G,\\Theta )\\pi (\\Theta \\vert G)}{{\\cal E}(x_t\\vert G)}\\,. \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In the spirit of attempting to match the behaviour of the process in as much detail as possible, but without explicit parametric modelling of it through more conventional means, let's now build a kernel convolution Gaussian process network. For some references on Gaussian processes, see here: \n",
    "\n",
    "- [Bayesian Time Series Learning with Gaussian Processes](http://mlg.eng.cam.ac.uk/pub/pdf/Fri15.pdf)\n",
    "- [Gaussian Process Networks](https://arxiv.org/abs/1301.3857)\n",
    "- [Deep Gaussian Processes](http://proceedings.mlr.press/v31/damianou13a.pdf) \n",
    "- [Bayesian Gaussian Process Latent Variable Model](http://proceedings.mlr.press/v9/titsias10a/titsias10a.pdf). \n",
    "\n",
    "This choice of approximation naturally imposes a restriction to the structure of ${\\rm DBN}(G,p)$ and hence on the equations above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `'AR-GP'`: a simple autoregressive kernel convolution Gaussian process network \n",
    "\n",
    "The point here was to design a very quick-to-use and robust method for generic time series prediction. In principle, more advanced methods/abstract causally-connected hidden layers could be compared to the benchmark performance of `'AR-GP'`, which relies on simply generating autoregressive features through kernel convolutions and then modelling a covariance between the resulting convolved signals.\n",
    "\n",
    "After standardisation of the training data $x_t\\rightarrow [x_t-{\\rm E}(x_t)]/[{\\rm Var}(x_t)]^{1/2}$ (and possibly differencing it $d_t=x_t-x_{t-1}$, or just some of it elements $d^i_t=x^i_t-x^i_{t-1}$ up to a certain order to obtain stationarity), the dynamic kernel convolution Gaussian process network models the whole vector process as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_{t} &\\sim {\\cal N}( x_{t}; m_{t} + f_t, V) \\\\\n",
    "m_{t} &\\sim {\\cal N}\\big[ m_{t};M_t(x_{t-1},x_{t-2},\\dots \\vert h) ,\\Sigma \\big] \\\\\n",
    "M^i_t(x^i_{t-1},x^i_{t-2},\\dots \\vert h^i) &= \\sum_{n=1}^{t-1}\\frac{x^i_{t-n}}{H^i}\\exp \\left[ -\\frac{A^i(n)}{(h^i)^2}\\right] \\,,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "H^i = \\sum_{n=1}^{t-1}\\exp \\left[ -\\frac{A^i(n)}{(h^i)^2}\\right]\\,, \\quad A^i(n) = \\begin{cases} \\frac{n^2}{2} \\,\\, ({\\rm and}\\,\\, f^i_t=0) &  {\\rm Squared}\\,\\,{\\rm exponential}\\\\ 2\\sin^2\\big( \\big\\vert \\frac{\\pi n}{n^i}\\big\\vert \\big) \\,\\, ({\\rm and}\\,\\, f^i_t=\\sin (\\frac{\\pi t}{n^i} + \\pi s^i) ) & {\\rm Periodic}\\end{cases}\\,, \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where ${\\cal N}(\\mu , \\Sigma )$ is a multivariate normal distribution with mean vector $\\mu$ and covariance matrix $\\Sigma$. The likelihood is therefore very simply\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\cal L}(x_t\\vert G,\\Theta ) = {\\cal N}\\big[ x_{t};\\tilde{M}_t(f_t,h),\\tilde{\\Sigma} \\big] \\,, \\quad \\tilde{M}_t(f_t,h) \\equiv f_t+M_t(x_{t-1},x_{t-2},\\dots \\vert h)\\,, \\quad  \\tilde{\\Sigma} \\equiv V+\\Sigma\\,. \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The graph displayed below illustrates the structure of this network, where shaded nodes are observed.\n",
    "\n",
    "<img src=\"simple_GP_network.png\" width=\"600\"/>\n",
    "\n",
    "It is clear that investigating the data for evidence of seasonality (by, e.g., examining the autocorrelation functions) with be an important first step before deciding on the convolution kernels connecting the input layer to the hidden layer. \n",
    "\n",
    "Not all of the graph edges should be strongly weighted by the data so we can (and should) select graph structures based on their combined Bayesian evidence over all of the past observations of the process $Z_t=\\prod_{n=1}^{t-1}{\\cal E}(x_{t-n}\\vert G)$. In order to convert the evaluation of $Z_t$ into an optimisation problem, we can choose an appropriate prior over $\\tilde{\\Sigma}$ that parameterises the family of posterior distributions. For a multivariate normal with fixed mean (assuming that the priors over $h$ and $f_t$ are independent) and unknown covariance, the conjugate prior is just the inverse-Wishart distribution ${\\cal W}^{-1}$ so from the definition of the posterior, we have simply\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P(x_t,\\Theta \\vert G ) \\propto {\\cal N}\\big[ x_{t};\\tilde{M}_t(f_t,h),\\tilde{\\Sigma} \\big]{\\cal W}^{-1}(\\tilde{\\Sigma};\\Psi , \\nu) \\quad \\Longleftrightarrow \\quad Z_t=\\prod_{n=1}^{t-1}{\\cal E}(x_{t-n}\\vert G) \\propto \\prod_{n=1}^{t-1}t_{\\nu - N +1}\\bigg[ x_{t-n};\\tilde{M}_{t-n}(f_t,h),\\frac{\\Psi}{\\nu - N + 1} \\bigg] \\,, \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $t_{\\nu}(\\mu , \\Sigma)$ is a multivariate t-distribution and the latter expression is obtained by marginalisation over the degrees of freedom in $\\tilde{\\Sigma}$. It is preferable at this point to define the priors over $h$ and $f_t$ as simply Dirac delta distributions centered on single parameter values (or $n^i$ and $s^i$ in the case of the $f_t^i$ functions) so that all of the epistemic uncertainty is propagated to the hidden-to-output layer weights. In addition, we will assume the degrees of freedom $\\nu = N$, corresponding to the non-informative prior over the covariance matrix. Under these prior assumptions one may replace the proportionalities above with exact equalities, which is how `'AR-GP'` works.\n",
    "\n",
    "In case a gradient descent algorithm is used to optimise $\\ln Z_t$, the first derivatives are\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\ln H^i}{\\partial (h^i)^2} &= \\frac{1}{H^i}\\sum_{n=1}^{t-1}\\frac{A^i(n)}{(h^i)^4}\\exp \\left[ -\\frac{A^i(n)}{(h^i)^2}\\right] \\\\\n",
    "\\frac{\\partial}{\\partial (h^i)^2}M^i_t(x^i_{t-1},x^i_{t-2},\\dots \\vert h^i) &= \\sum_{n=1}^{t-1} \\bigg[ \\frac{A^i(n)}{(h^i)^4} - \\frac{\\partial \\ln H^i}{\\partial (h^i)^2}\\bigg] \\frac{x^i_{t-n}}{H^i} \\exp \\left[ -\\frac{A^i(n)}{(h^i)^2}\\right] \\\\\n",
    "&= \\sum_{n=1}^{t-1} \\frac{A^i(n)}{(h^i)^4} \\frac{x^i_{t-n}}{H^i} \\exp \\left[ -\\frac{A^i(n)}{(h^i)^2}\\right] - \\frac{\\partial \\ln H^i}{\\partial (h^i)^2}M^i_t(x^i_{t-1},x^i_{t-2},\\dots \\vert h^i) \\\\\n",
    "\\ln Z_t &= \\ln \\Gamma \\bigg(\\frac{\\nu + 1}{2}\\bigg) - \\ln\\Gamma \\bigg(\\frac{\\nu - N + 1}{2}\\bigg) - \\frac{N}{2}\\ln ( \\pi ) - \\frac{1}{2}\\ln {\\rm det} ( \\Psi ) \\\\\n",
    "&- \\frac{\\nu + 1}{2}\\sum_{n=1}^{t-1}\\ln \\bigg\\{ 1+\\big[ x_{t-n}-\\tilde{M}_{t-n}(f_t,h) \\big]^{\\rm T} \\Psi^{-1}\\big[ x_{t-n}-\\tilde{M}_{t-n}(f_t,h) \\big] \\bigg\\} \\\\\n",
    "\\frac{\\partial \\ln Z_t}{\\partial (h^i)^2} &= - (\\nu + 1)\\sum_{n=1}^{t-1}\\bigg\\{ 1+\\big[ x_{t-n}-\\tilde{M}_{t-n}(f_t,h) \\big]^{\\rm T} \\Psi^{-1}\\big[ x_{t-n}-\\tilde{M}_{t-n}(f_t,h) \\big] \\bigg\\}^{-1} \\\\\n",
    "&\\qquad \\qquad \\qquad \\times \\big[ \\frac{\\partial}{\\partial (h^i)^2}M^i_t(x^i_{t-1},x^i_{t-2},\\dots \\vert h^i) \\big] \\sum_{j=0}^N\\big( \\Psi^{-1}\\big)^i_j\\big[ x^j_{t-n}-\\tilde{M}^j_{t-n}(f^j_t,h^j) \\big] \\\\\n",
    "\\frac{\\partial \\ln Z_t}{\\partial \\Psi^i_j} &= - \\frac{1}{2}\\delta^i_j + \\frac{\\nu + 1}{2}\\sum_{n=1}^{t-1} \\bigg\\{ 1+\\big[ x_{t-n}-\\tilde{M}_{t-n}(f_t,h) \\big]^{\\rm T} \\Psi^{-1}\\big[ x_{t-n}-\\tilde{M}_{t-n}(f_t,h) \\big] \\bigg\\}^{-1}\\\\\n",
    "&\\qquad \\qquad \\qquad \\qquad \\times \\big[ x^i_{t-n}-\\tilde{M}^i_{t-n}(f^i_t,h^i) \\big] \\big( \\Psi^{-2}\\big)^i_j\\big[ x^j_{t-n}-\\tilde{M}^j_{t-n}(f^j_t,h^j) \\big]\\,.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do:\n",
    "### - Add some empirical Bayes theory\n",
    "### - Add functionality to deal with oscillatory signals to the `bants` class\n",
    "### - Add SGD method to the `bants` class and check it works\n",
    "### - Suggest (and try out) other sklearn models in the mean field layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
