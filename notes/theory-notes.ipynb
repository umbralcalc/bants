{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BANTS theory documentation\n",
    "\n",
    "### Dynamic Bayesian network structure\n",
    "\n",
    "A dynamic Bayesian network is denoted ${\\rm DBN}(G,p)$ where $G$ denotes the graph structure and $p$ denotes the joint probability distribution over vertices. The latter, for the observation vector of an $N$-dimensional time series process $x_{t}$ (with $i$ indexing an element of the vector $x$, and $t$ denoting a discrete observation timepoint index) is factorisable with respect to the graph structure in the following manner \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(x_{t},x_{t-1},\\dots ,x_{1}) = \\prod_{n=0}^{t-1}p[x_{t-n}\\vert {\\rm pa}(x_{t-n})]\\,,\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where ${\\rm pa}(x_{t})$ denotes the parent nodes of $x_{t}$ which are defined by $G$. Choosing a set of parameters $\\Theta$ (defined within their prior domain $\\Omega_\\Theta$), graph $G$, and an observation of $x_t$, Bayes' rule yields the following for the posterior distribution over $\\Theta$ at timepoint $t$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\cal P}_G[\\Theta \\vert {\\rm pa}(x_{t})] = \\frac{P_G[{\\rm pa}(x_{t}),\\Theta ]}{\\int_{\\Theta \\in \\Omega_\\Theta} {\\rm d}\\Theta P_G[{\\rm pa}(x_{t}),\\Theta ]} = \\frac{{\\cal L}_G[{\\rm pa}(x_{t})\\vert \\Theta ]\\pi_G (\\Theta )}{{\\cal E}_G[{\\rm pa}(x_{t})]}\\,,\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where, as usual, ${\\cal L}_G$ is the likelihood, $\\pi_G$ is the prior and ${\\cal E}_G$ is the evidence. \n",
    "\n",
    "In [empirical Bayes](https://en.wikipedia.org/wiki/Empirical_Bayes_method), one defines an auxiliary time-dependent vector variable $m_t$ (which $\\pi_G$ and ${\\cal E}_G$ are typically conditioned on) to marginalise over in this posterior definition like so \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\cal P}_G[\\Theta \\vert {\\rm pa}(x_{t})] = \\int_{m \\in \\Omega_{m_t}} {\\rm d}m \\, P_G[m\\vert {\\rm pa}(x_{t})]\\frac{{\\cal L}_G[{\\rm pa}(x_{t})\\vert \\Theta ]\\pi_G (\\Theta \\vert m)}{{\\cal E}_G[{\\rm pa}(x_{t}) \\vert m]} \\,.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The vector variable $m_t$, which will form an intermediate layer in our network, acts as a compression of ${\\rm pa}(x_{t})$ at time $t$ which we will importantly assume to be _Markovian_ (note that this assumption will not always hold well for the best compressions). In the `bants` class, the likelihood of $m_t$ is further restricted by assuming it is drawn from a Gaussian process. For some references on Gaussian processes, see, e.g., [Friedman & Nachman (2000)](https://arxiv.org/abs/1301.3857), [Titsias & Lawrence (2010)](http://proceedings.mlr.press/v9/titsias10a/titsias10a.pdf), [Damianou & Lawrence (2013)](http://proceedings.mlr.press/v31/damianou13a.pdf) or [Frigola-Alcalde (2015)](http://mlg.eng.cam.ac.uk/pub/pdf/Fri15.pdf). \n",
    "\n",
    "Up until this point, the technique used for compressing ${\\rm pa}(x_{t}) \\rightarrow m_t$ is still undetermined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `'AR-GP'`: a simple autoregressive kernel convolution Gaussian process network \n",
    "\n",
    "A simple nonparametric way to obtain $m_t$ is through autoregressive techniques with convolution kernels. After standardisation of the training data $x_t\\rightarrow [x_t-{\\rm E}(x_t)]/[{\\rm Var}(x_t)]^{1/2}$ (and possibly differencing it $d_t=x_t-x_{t-1}$, or just some of it elements $d^i_t=x^i_t-x^i_{t-1}$ up to a certain order to obtain stationarity), the dynamic kernel convolution Gaussian process network models the whole vector process as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_{t} &\\sim {\\cal N}( x_{t}; m_{t} + f_t, V) \\\\\n",
    "m_{t} &\\sim {\\cal N}\\big\\{ m_{t};M_t[{\\rm pa}(x_{t}) \\vert h] ,\\Sigma \\big\\} \\\\\n",
    "M^i_t[{\\rm pa}(x^i_{t}) \\vert h^i] &= \\sum_{n=1}^{t-1}\\frac{x^i_{t-n}}{H^i}\\exp \\left[ -\\frac{A^i(n)}{(h^i)^2}\\right] \\,,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "H^i = \\sum_{n=1}^{t-1}\\exp \\left[ -\\frac{A^i(n)}{(h^i)^2}\\right]\\,, \\quad A^i(n) = \\begin{cases} \\frac{n^2}{2} \\,\\, ({\\rm and}\\,\\, f^i_t=0) &  {\\rm Squared}\\,\\,{\\rm exponential}\\\\ 2\\sin^2\\big( \\big\\vert \\frac{\\pi n}{n^i}\\big\\vert \\big) \\,\\, ({\\rm and}\\,\\, f^i_t=\\sin (\\frac{\\pi t}{n^i} + \\pi s^i) ) & {\\rm Periodic}\\end{cases}\\,, \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where ${\\cal N}(\\mu , \\Sigma )$ is a multivariate normal distribution with mean vector $\\mu$ and covariance matrix $\\Sigma$. The likelihood is therefore very simply\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\cal L}_G[{\\rm pa}(x_{t})\\vert \\Theta ] = {\\cal N}\\big[ x_{t};\\tilde{M}_t(f_t,h),\\tilde{\\Sigma} \\big] \\,, \\quad \\tilde{M}_t(f_t,h) \\equiv f_t+M_t[{\\rm pa}(x_{t}) \\vert h]\\,, \\quad  \\tilde{\\Sigma} \\equiv V+\\Sigma\\,. \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The graph displayed below illustrates the structure of this network, where shaded nodes are observed.\n",
    "\n",
    "<img src=\"simple_GP_network.png\" width=\"600\"/>\n",
    "\n",
    "It is clear that investigating the data for evidence of seasonality (by, e.g., examining the autocorrelation functions) with be an important first step before deciding on the convolution kernels connecting the input layer to the hidden layer. \n",
    "\n",
    "Not all of the graph edges should be strongly weighted by the data so we can (and should) select graph structures based on their combined Bayesian evidence over all of the past observations of the process $Z_t=\\prod_{n=0}^{t-1}{\\cal E}_G[{\\rm pa}(x_{t-n})]$. In order to convert the evaluation of $Z_t$ into an optimisation problem, we can choose an appropriate prior over $\\tilde{\\Sigma}$ that parameterises the family of posterior distributions. For a multivariate normal with fixed mean (assuming that the priors over $h$ and $f_t$ are independent) and unknown covariance, the conjugate prior is just the inverse-Wishart distribution ${\\cal W}^{-1}$ so from the definition of the posterior, we have simply\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P_G[{\\rm pa}(x_{t}),\\Theta ] \\propto {\\cal N}\\big[ x_{t};\\tilde{M}_t(f_t,h),\\tilde{\\Sigma} \\big]{\\cal W}^{-1}(\\tilde{\\Sigma};\\Psi , \\nu) \\quad \\Longleftrightarrow \\quad Z_t=\\prod_{n=0}^{t-1}{\\cal E}_G[{\\rm pa}(x_{t-n})] \\propto \\prod_{n=0}^{t-1}{\\sf t}_{\\nu - N +1}\\bigg[ x_{t-n};\\tilde{M}_{t-n}(f_t,h),\\frac{\\Psi}{\\nu - N + 1} \\bigg] \\,, \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where ${\\sf t}_{\\nu}(\\mu , \\Sigma)$ is a multivariate t-distribution and the latter expression is obtained by marginalisation over the degrees of freedom in $\\tilde{\\Sigma}$. It is preferable at this point to define the priors over $h$ and $f_t$ as simply Dirac delta distributions centered on single parameter values (or $n^i$ and $s^i$ in the case of the $f_t^i$ functions) so that all of the epistemic uncertainty is propagated to the hidden-to-output layer weights. Using this prior one may replace the proportionalities above with exact equalities, which correspond to the method that is actually implemented within the `bants` class. Note also that the default setting for the degrees of freedom $\\nu = N$, which corresponds to the non-informative prior over the covariance matrix - though this can be varied and/or optimised separately.\n",
    "\n",
    "In case a gradient descent algorithm is used to optimise $\\ln Z_t$, it first derivatives (and other relevant quantities) are\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\ln H^i}{\\partial (h^i)^2} &= \\frac{1}{H^i}\\sum_{n=1}^{t-1}\\frac{A^i(n)}{(h^i)^4}\\exp \\left[ -\\frac{A^i(n)}{(h^i)^2}\\right] \\\\\n",
    "\\frac{\\partial}{\\partial (h^i)^2}M^i_t[{\\rm pa}(x^i_{t}) \\vert h^i] &= \\sum_{n=1}^{t-1} \\bigg[ \\frac{A^i(n)}{(h^i)^4} - \\frac{\\partial \\ln H^i}{\\partial (h^i)^2}\\bigg] \\frac{x^i_{t-n}}{H^i} \\exp \\left[ -\\frac{A^i(n)}{(h^i)^2}\\right] \\\\\n",
    "&= \\sum_{n=1}^{t-1} \\frac{A^i(n)}{(h^i)^4} \\frac{x^i_{t-n}}{H^i} \\exp \\left[ -\\frac{A^i(n)}{(h^i)^2}\\right] - \\frac{\\partial \\ln H^i}{\\partial (h^i)^2}M^i_t[{\\rm pa}(x^i_{t}) \\vert h^i] \\\\\n",
    "\\ln Z_t &= \\ln \\Gamma \\bigg(\\frac{\\nu + 1}{2}\\bigg) - \\ln\\Gamma \\bigg(\\frac{\\nu - N + 1}{2}\\bigg) - \\frac{N}{2}\\ln ( \\pi ) - \\frac{1}{2}\\ln {\\rm det} ( \\Psi ) \\\\\n",
    "&- \\frac{\\nu + 1}{2}\\sum_{n=1}^{t-1}\\ln \\bigg\\{ 1+\\big[ x_{t-n}-\\tilde{M}_{t-n}(f_t,h) \\big]^{\\rm T} \\Psi^{-1}\\big[ x_{t-n}-\\tilde{M}_{t-n}(f_t,h) \\big] \\bigg\\} \\\\\n",
    "\\frac{\\partial \\ln Z_t}{\\partial (h^i)^2} &= - (\\nu + 1)\\sum_{n=1}^{t-1}\\bigg\\{ 1+\\big[ x_{t-n}-\\tilde{M}_{t-n}(f_t,h) \\big]^{\\rm T} \\Psi^{-1}\\big[ x_{t-n}-\\tilde{M}_{t-n}(f_t,h) \\big] \\bigg\\}^{-1} \\\\\n",
    "&\\qquad \\qquad \\qquad \\times \\big[ \\frac{\\partial}{\\partial (h^i)^2}M^i_t(x^i_{t-1},x^i_{t-2},\\dots \\vert h^i) \\big] \\sum_{j=0}^N\\big( \\Psi^{-1}\\big)^i_j\\big[ x^j_{t-n}-\\tilde{M}^j_{t-n}(f^j_t,h^j) \\big] \\\\\n",
    "\\frac{\\partial \\ln Z_t}{\\partial \\Psi^i_j} &= - \\frac{1}{2}\\delta^i_j + \\frac{\\nu + 1}{2}\\sum_{n=1}^{t-1} \\bigg\\{ 1+\\big[ x_{t-n}-\\tilde{M}_{t-n}(f_t,h) \\big]^{\\rm T} \\Psi^{-1}\\big[ x_{t-n}-\\tilde{M}_{t-n}(f_t,h) \\big] \\bigg\\}^{-1}\\\\\n",
    "&\\qquad \\qquad \\qquad \\qquad \\times \\big[ x^i_{t-n}-\\tilde{M}^i_{t-n}(f^i_t,h^i) \\big] \\big( \\Psi^{-2}\\big)^i_j\\big[ x^j_{t-n}-\\tilde{M}^j_{t-n}(f^j_t,h^j) \\big]\\,.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `'ML-GP'`: a simple Gaussian process network over machine-learned mean field signals\n",
    "\n",
    "Defining $I(X;Y)$ as the mutual information between $X$ and $Y$, the information bottleneck (see [Tishby, Pereira & Bialek (1999)](https://www.cs.huji.ac.il/labs/learning/Papers/allerton.pdf)) Lagrangian in our case becomes\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L_t &= \\sum_{n=0}^{t-1}\\big\\{ I[{\\rm pa}(x_{t-n});m_{t-n}] -\\beta I(m_{t-n};x_{t-n})\\big\\} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do:\n",
    "#### - Do detailed theoretical work with the information bottleneck to show how this can be used to get $m_t$ in general prior to using the `'ML-GP'` network model\n",
    "#### - Add functionality to deal with oscillatory signals to the `bants` class and show a worked example\n",
    "#### - Add functionality to use arbitrary models with a `.predict` method as (e.g., sklearn) models in the mean field layer to be optimised wrt hyperparams according to a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
