{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example `AR-G` model application to mock time series data \n",
    "\n",
    "Let's demonstrate how to use the `AR-G` model in the `bants` class on a simple mock example. First we need to import `bants` and other modules to make the mock data and do some plotting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '../../bants/' # Give your path to bants here\n",
    "sys.path.append(path + 'source/')\n",
    "from bants import bants\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate some mock time series data using a random Langevin system..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolve the n-d Langevin system\n",
    "def L_syst_gen(n,runtime,stepsize):    \n",
    "    # Specify simple drift function with random linear couplings\n",
    "    def f(x):\n",
    "        a = np.random.normal(0.0,1.0,size=len(x))\n",
    "        b = np.random.normal(0.0,1.0,size=len(x))\n",
    "        return a*x + b*x[np.random.randint(0,len(x),size=len(x))]    \n",
    "    # Initialise process and run\n",
    "    t, x = 0.0, np.random.normal(0.0,1.0,size=n)\n",
    "    out_store = [] \n",
    "    while t < runtime:\n",
    "        t += stepsize\n",
    "        x += -(stepsize*f(x))+(np.sqrt(stepsize)*np.random.normal(0.0,1.0,size=n))\n",
    "        out_store.append(np.append(t,x))        \n",
    "    # Output time series as a pandas dataframe\n",
    "    out_df = pd.DataFrame(np.asarray(out_store),columns=['t']+['x'+str(ni) for ni in range(0,n)]).set_index('t')\n",
    "    return out_df\n",
    "# Choose number of dimensions, total runtime and stepsize\n",
    "n = 5\n",
    "runtime = 10.0\n",
    "stepsize = 0.01\n",
    "# Run Langevin system and plot output\n",
    "df = L_syst_gen(n,runtime,stepsize)\n",
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first apply differencing to the data as a pre-processing step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = df.diff().iloc[1:]\n",
    "ddf.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bants` class works in the same pattern as a `scikit-learn` estimator. We initialise `bants('AR-G')` and fit with the `bants.fit` method to the differenced dataframe with the in-built optimiser methods..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise class with 'AR-G' model type (subsequently comment out if iterative runs of the cell are desired)\n",
    "bts = bants('AR-G')\n",
    "# Set the maximum number of algorithm iterations per fitting run and the optimiser - it \n",
    "# works quite well to start with 'GD' then finish off with 'Nelder-Mead' near the optimum\n",
    "bts.optimiser = 'Nelder-Mead' # 'Nelder-Mead' is also an option\n",
    "bts.params['learn_rate'] = 0.001 # Only relevant for 'GD'\n",
    "bts.params[\"itmax\"] = 100\n",
    "nfunc_eval = 0\n",
    "lnE_vals = []\n",
    "for i in range(0,10):\n",
    "    # Apply 'fit' method on the training dataframe to optimise Bayesian model hyperparameters where\n",
    "    # standardisation of the training data is automatically performed unless the keyword standard=False\n",
    "    bts.fit(ddf)\n",
    "    # Update initial parameter values for next run\n",
    "    bts.hsq_guess = bts.params['hsq']\n",
    "    bts.Psi_tril_guess = bts.params['Psi_tril']\n",
    "    # Store the log-evidence values from the run and add to the total number of function evaluations\n",
    "    lnE_vals.append(bts.info['lnE_val'])\n",
    "    nfunc_eval += bts.info['n_evaluations']\n",
    "# Display the best fit hyperparameters\n",
    "print('Best fit hyperparameters are: ' + str(bts.params))\n",
    "# Display the best fit log-evidence value\n",
    "print('Best fit log-evidence value is: ' + str(bts.info['lnE_val']))\n",
    "# Ask if the fitting converged\n",
    "print('Converged?: ' + str(bts.info['converged']))\n",
    "# Display the number of function evaluations\n",
    "print('Number of function evaluations: ' + str(nfunc_eval))\n",
    "# Plot intermediate values and base-10 logarithmic improvements in the log-evidence after each run\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.plot(lnE_vals,color='b')\n",
    "ax2.plot(np.log10(np.asarray(lnE_vals)[1:]-np.asarray(lnE_vals)[:-1]),color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the `bants.predict` method, the fitted Bayesian graphical model can be used to generate predictive samples into the future..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future timepoint to sample up to\n",
    "future_t = 10.3\n",
    "# Number of prediction samples to generate at each timepoint\n",
    "n_samples = 1000\n",
    "# Apply bants.predict to forecast time series up to future time \n",
    "# from training data last timepoint index using the MAP of the graphical model\n",
    "pred_samps = bts.predict(future_t, n_samples, compute_map=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and we can now plot the predictive contours..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store colour palette\n",
    "cp = sns.color_palette()\n",
    "# Loop over dimensions of time series for plot\n",
    "for i in range(0,5):\n",
    "    # Cumulative sum of samples (integration) to remove differencing (remember to append training points to this!)\n",
    "    fp = np.tensordot(df.values[0,i]*np.ones(1),np.ones(n_samples),axes=0) # First point\n",
    "    dtd = np.tensordot(ddf.values[:,i],np.ones(n_samples),axes=0) # Differenced training data\n",
    "    diff_preds = np.append(fp,np.append(dtd,pred_samps[:,i,:],axis=0),axis=0) # Append along time axis\n",
    "    isamps = np.cumsum(diff_preds,axis=0)[-pred_samps.shape[0]:]\n",
    "    # Compute 68% confidence\n",
    "    c68l, c68u = np.quantile(isamps,[0.16,0.84],axis=1)\n",
    "    # Use these to plot contours\n",
    "    plt.plot(df.index,df.values[:,i],color=cp[i])\n",
    "    plt.fill_between(np.arange(runtime+stepsize,future_t,stepsize),c68l,c68u,alpha=0.4,color=cp[i])\n",
    "# Modify time axis for clearer display\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([(2*runtime)-future_t,future_t])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
